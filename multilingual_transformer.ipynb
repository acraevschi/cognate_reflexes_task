{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, take note that folder ```ST2022``` contains only data and surprise data. The rest of the files were removed. You would have to install ```ST2022``` official package to run the function used at the end that evaluates the prediction against the real target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    TextVectorization,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras import layers\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell reads the data in a dictionary. The changes needed to modify condition are highlighted with a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_dirs = [ # Training data\n",
    "    \"abrahammonpa\",\n",
    "    \"allenbai\",\n",
    "    \"backstromnorthernpakistan\",\n",
    "    \"castrosui\",\n",
    "    \"davletshinaztecan\",\n",
    "    \"felekesemitic\",\n",
    "    \"hantganbangime\",\n",
    "    \"hattorijaponic\",\n",
    "    \"listsamplesize\",\n",
    "    \"mannburmish\",\n",
    "]\n",
    "\n",
    "### Uncomment the following list for surprise data\n",
    "# lst_dirs = [ \n",
    "#     \"bantubvd\",\n",
    "#     \"beidazihui\",\n",
    "#     \"birchallchapacuran\",\n",
    "#     \"bodtkhobwa\",\n",
    "#     \"bremerberta\",\n",
    "#     \"deepadungpalaung\",\n",
    "#     \"hillburmish\",\n",
    "#     \"kesslersignificance\",\n",
    "#     \"luangthongkumkaren\",\n",
    "#     \"wangbai\"\n",
    "# ]\n",
    "\n",
    "max_num_lang = 0\n",
    "max_seq_len = 0\n",
    "num_langs = set()\n",
    "data_dict = {}\n",
    "val_data_dict = {}\n",
    "for dir in lst_dirs:\n",
    "    with open(\n",
    "        \"../ST2022/data/\" + dir + \"/training-0.10.tsv\", # change to training-0.50.tsv for 50% proportion\n",
    "        encoding=\"UTF-8\",\n",
    "    ) as f:\n",
    "        file = f.readlines()\n",
    "        data = list(map(lambda x: x.strip(\"\\n\").split(\"\\t\")[1:], file))\n",
    "        \n",
    "        val_num = int(len(data) * 0.025)\n",
    "        \n",
    "        start_index = random.randint(0, len(data)-val_num)\n",
    "        end_index = start_index + val_num\n",
    "        train_data = data[:start_index] + data[end_index:]\n",
    "        val_data = data[start_index:end_index]\n",
    "\n",
    "    header = data[0]\n",
    "    for i, head in enumerate(header):\n",
    "        for data_point in train_data[1:]:\n",
    "            if (\n",
    "                data_point[i] == \"\"\n",
    "            ):  # dictionary keys are strings with target language and target form. If there is not target form, skip the iteration\n",
    "                continue\n",
    "            else:\n",
    "                offset_target_form = f\"[start] {head} \" + data_point[i] + \" [end]\"\n",
    "\n",
    "                data_dict[offset_target_form] = [\n",
    "                    f\"{header[k]} {data_point[k]}\"\n",
    "                    for k in range(len(data_point))\n",
    "                    if k != i\n",
    "                ]\n",
    "\n",
    "                for form in data_dict[offset_target_form]:\n",
    "                    if len(form.split()) > max_seq_len:\n",
    "                        max_seq_len = len(form.split())\n",
    "        for data_point in val_data:\n",
    "            if (\n",
    "                data_point[i] == \"\"\n",
    "            ):  # dictionary keys are strings with target language and target form. If there is not target form, skip the iteration\n",
    "                continue\n",
    "            else:\n",
    "                offset_target_form = f\"[start] {head} \" + data_point[i] + \" [end]\"\n",
    "                val_data_dict[offset_target_form] = [\n",
    "                    f\"{header[k]} {data_point[k]}\"\n",
    "                    for k in range(len(data_point))\n",
    "                    if k != i\n",
    "                ]\n",
    "                for form in val_data_dict[offset_target_form]:\n",
    "                    if len(form.split()) > max_seq_len:\n",
    "                        max_seq_len = len(form.split())\n",
    "                        \n",
    "max_seq_len += 5\n",
    "target_lang = []\n",
    "target_lang_form = []\n",
    "neighbor_lang_form = []\n",
    "# neighbor_lang_form_augment = []\n",
    "\n",
    "val_target_lang = []\n",
    "val_target_lang_form = []\n",
    "val_neighbor_lang_form = []\n",
    "\n",
    "\n",
    "for key, value in data_dict.items():\n",
    "    target_lang_form.append(key)\n",
    "    neighbor_lang_form.append(value)\n",
    "    target_lang.append(\n",
    "        key.split()[1]\n",
    "    )  # the first element in the sequence of targets is [start] followed by language code\n",
    "\n",
    "for key, value in val_data_dict.items():\n",
    "    val_target_lang_form.append(key)\n",
    "    val_neighbor_lang_form.append(value)\n",
    "    val_target_lang.append(\n",
    "        key.split()[1]\n",
    "    )  # the first element in the sequence of targets is [start] followed by language code\n",
    "\n",
    "\n",
    "max_num_lang = max([len(langs) for langs in neighbor_lang_form])  # needed for padding\n",
    "\n",
    "all_langs = list(set(target_lang))\n",
    "all_forms = list(\n",
    "    set(\n",
    "        [\n",
    "            char\n",
    "            for lst in neighbor_lang_form + val_neighbor_lang_form\n",
    "            for seq in lst\n",
    "            for char in seq.split()\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "form_vectorization = TextVectorization(\n",
    "    standardize=None,\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_seq_len + 1,\n",
    ")\n",
    "\n",
    "lang_vectorization = TextVectorization(\n",
    "    standardize=None,\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=1,\n",
    ")\n",
    "\n",
    "lang_vectorization.adapt(all_langs)\n",
    "### Vectorize target languages\n",
    "int_target_lang = lang_vectorization(target_lang)\n",
    "val_int_target_lang = lang_vectorization(val_target_lang)\n",
    "\n",
    "form_vectorization.adapt(all_forms)\n",
    "### Vectorize target sequence\n",
    "int_target_lang_form = form_vectorization(target_lang_form)\n",
    "val_int_target_lang_form = form_vectorization(val_target_lang_form)\n",
    "\n",
    "### Prepare lists for vectorization and padding of neighbor forms\n",
    "int_neighbor_lang_form = []\n",
    "val_int_neighbor_lang_form = []\n",
    "\n",
    "\n",
    "vocab_size_forms = form_vectorization.vocabulary_size()\n",
    "vocab_size_langs = lang_vectorization.vocabulary_size()\n",
    "\n",
    "lang_form_vocab = form_vectorization.get_vocabulary()\n",
    "lang_form_lookup = dict(zip(range(len(lang_form_vocab)), lang_form_vocab))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell take long time to process, as it's poorly optimized for now. The waiting time can be avoided by downloading the zipped ```tf.data.Dataset``` corresponding to configuration of interest. \n",
    "\n",
    "- Loop 1 vectorizes training neighbor forms and then padds them with sequences of zeros up to the required shape (18, 19)\n",
    "- Loop 2 does the same for validation neighbor forms\n",
    "- Loop 3 performs the data augmentation as described in the paper. As the data becomes three times the original dataset, target forms and target languages are also copied three times\n",
    "- Loop 4 does the same for validation data\n",
    "\n",
    "By chance, few cognate sets might get completely deleted and will thus be full of zeros. To avoid it, function ```filter_zeros``` is applied afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Loop 1 \n",
    "for neighborhood in tqdm(neighbor_lang_form, desc=f\"Processing neighborhoods\"):\n",
    "    int_neighborhood = form_vectorization(neighborhood)\n",
    "    num_neighbors = int_neighborhood.shape[0]\n",
    "    if num_neighbors < max_num_lang:\n",
    "        to_add = max_num_lang - num_neighbors\n",
    "        padding = tf.constant([[0, to_add], [0, 0]], dtype=tf.int32)\n",
    "        int_neighborhood = tf.pad(int_neighborhood, padding)\n",
    "    int_neighbor_lang_form.append(int_neighborhood)\n",
    "\n",
    "# Loop 2\n",
    "for neighborhood in tqdm(val_neighbor_lang_form, desc=f\"Processing validation neighborhoods\"):\n",
    "    int_neighborhood = form_vectorization(neighborhood)\n",
    "    num_neighbors = int_neighborhood.shape[0]\n",
    "    if num_neighbors < max_num_lang:\n",
    "        to_add = max_num_lang - num_neighbors\n",
    "        padding = tf.constant([[0, to_add], [0, 0]], dtype=tf.int32)\n",
    "        int_neighborhood = tf.pad(int_neighborhood, padding)\n",
    "    val_int_neighbor_lang_form.append(int_neighborhood)\n",
    "\n",
    "int_neighbor_lang_form = tf.convert_to_tensor(int_neighbor_lang_form)\n",
    "val_int_neighbor_lang_form = tf.convert_to_tensor(val_int_neighbor_lang_form)\n",
    "\n",
    "    # Loop 3\n",
    "augmented_data = []\n",
    "other_augmented_data = []\n",
    "zero_tensor = tf.zeros(shape=(20,), dtype=tf.int64)\n",
    "for neighborhood in tqdm(int_neighbor_lang_form, desc=f\"Processing augmented neighborhoods\"):\n",
    "    augmented_neighborhood = []\n",
    "    other_augmented_neighborhood = []\n",
    "    for neighbor in neighborhood:\n",
    "        if all(neighbor == zero_tensor):\n",
    "            augmented_neighborhood.append(zero_tensor)\n",
    "            other_augmented_neighborhood.append(zero_tensor)\n",
    "        elif random.randint(0, 1) == 1:\n",
    "            augmented_neighborhood.append(neighbor)\n",
    "            other_augmented_neighborhood.append(zero_tensor)\n",
    "        else:\n",
    "            augmented_neighborhood.append(zero_tensor)\n",
    "            other_augmented_neighborhood.append(neighbor)\n",
    "    augmented_data.append(augmented_neighborhood)\n",
    "    other_augmented_data.append(other_augmented_neighborhood)\n",
    "augmented_data = tf.convert_to_tensor(augmented_data + other_augmented_data)\n",
    "\n",
    "int_target_lang = tf.concat([int_target_lang, int_target_lang, int_target_lang], axis=0)\n",
    "int_target_lang_form = tf.concat([int_target_lang_form, int_target_lang_form, int_target_lang_form], axis=0)\n",
    "int_neighbor_lang_form = tf.concat([int_neighbor_lang_form, augmented_data], axis=0)\n",
    "\n",
    "\n",
    "augmented_data = []\n",
    "other_augmented_data = []\n",
    "zero_tensor = tf.zeros(shape=(20,), dtype=tf.int64)\n",
    "for neighborhood in tqdm(val_int_neighbor_lang_form, desc=f\"Processing augmented validation neighborhoods\"):\n",
    "    augmented_neighborhood = []\n",
    "    other_augmented_neighborhood = []\n",
    "    for neighbor in neighborhood:\n",
    "        if all(neighbor == zero_tensor):\n",
    "            augmented_neighborhood.append(zero_tensor)\n",
    "            other_augmented_neighborhood.append(zero_tensor)\n",
    "        elif random.randint(0, 1) == 1:\n",
    "            augmented_neighborhood.append(neighbor)\n",
    "            other_augmented_neighborhood.append(zero_tensor)\n",
    "        else:\n",
    "            augmented_neighborhood.append(zero_tensor)\n",
    "            other_augmented_neighborhood.append(neighbor)\n",
    "    augmented_data.append(augmented_neighborhood)\n",
    "    other_augmented_data.append(other_augmented_neighborhood)\n",
    "augmented_data = tf.convert_to_tensor(augmented_data + other_augmented_data)\n",
    "\n",
    "val_int_target_lang = tf.concat([val_int_target_lang, val_int_target_lang, val_int_target_lang], axis=0)\n",
    "val_int_target_lang_form = tf.concat([val_int_target_lang_form, val_int_target_lang_form, val_int_target_lang_form], axis=0)\n",
    "val_int_neighbor_lang_form = tf.concat([val_int_neighbor_lang_form, augmented_data], axis=0)\n",
    "\n",
    "\n",
    "def filter_zeros(example, target):\n",
    "    neighbor_langs_forms = example[\"neighbor_langs_forms\"]\n",
    "    all_zeros = tf.reduce_all(tf.equal(neighbor_langs_forms, 0))\n",
    "    return tf.logical_not(all_zeros)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\n",
    "            \"neighbor_langs_forms\": int_neighbor_lang_form[:, :, :-1],\n",
    "            \"target_langs\": int_target_lang,\n",
    "            \"target_langs_forms\": int_target_lang_form[:, :-1],\n",
    "        },\n",
    "    int_target_lang_form[:, 1:],\n",
    "    )\n",
    ")\n",
    "train_dataset = train_dataset.filter(filter_zeros)\n",
    "\n",
    "### Uncomment the following line to save the dataset\n",
    "# train_dataset.save(\"train_dataset_prop_0.10\")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\n",
    "            \"neighbor_langs_forms\": val_int_neighbor_lang_form[:, :, :-1],\n",
    "            \"target_langs\": val_int_target_lang,\n",
    "            \"target_langs_forms\": val_int_target_lang_form[:, :-1],\n",
    "        },\n",
    "        val_int_target_lang_form[:, 1:],\n",
    "    )\n",
    ")\n",
    "val_dataset = val_dataset.filter(filter_zeros)\n",
    "\n",
    "### Uncomment the following line to save the dataset\n",
    "# val_dataset.save(\"val_dataset_prop_0.10\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the following cell, two lines can be uncommented. These two lines load the datasets from saved folders, in case you haven't run the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 75000\n",
    "batch_size = 32\n",
    "\n",
    "# val_dataset = tf.data.Dataset.load(\"val_dataset_prop_0.10\") # uncomment to load\n",
    "validation_dataset_batched = val_dataset.batch(batch_size=batch_size)\n",
    "\n",
    "# train_dataset = tf.data.Dataset.load(\"train_dataset_prop_0.10\") # uncomment to load\n",
    "train_dataset_batched = (\n",
    "    train_dataset.shuffle(buffer_size=buffer_size).batch(batch_size=batch_size).prefetch(tf.data.AUTOTUNE).cache()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell subclasses Keras' ```Layer``` class to create a simple stack of encoder/decoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class Encoder(keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, num_heads, hidden_dim, dropout_rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_layers = [\n",
    "            keras_nlp.layers.TransformerEncoder(\n",
    "                intermediate_dim=hidden_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout_rate,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoder_layers[i](x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Encoder, self).get_config()\n",
    "        config[\"num_layers\"] = self.num_layers\n",
    "        config[\"num_heads\"] = self.encoder_layers[0].num_heads\n",
    "        config[\"hidden_dim\"] = self.encoder_layers[0].intermediate_dim\n",
    "        config[\"dropout_rate\"] = self.encoder_layers[0].dropout\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, num_heads, hidden_dim, dropout_rate, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.decoder_layers = [\n",
    "            keras_nlp.layers.TransformerDecoder(\n",
    "                intermediate_dim=hidden_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout_rate,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, context):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.decoder_layers[i](decoder_sequence=x, encoder_sequence=context)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Decoder, self).get_config()\n",
    "        config[\"num_layers\"] = self.num_layers\n",
    "        config[\"num_heads\"] = self.decoder_layers[0].num_heads\n",
    "        config[\"hidden_dim\"] = self.decoder_layers[0].intermediate_dim\n",
    "        config[\"dropout_rate\"] = self.decoder_layers[0].dropout\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    embeeding_dim_lang = 64\n",
    "    embedding_dim_form = 192\n",
    "    embed_drop = 0.15\n",
    "    dense_dim_langs = 512\n",
    "    num_heads_langs = 4\n",
    "    num_layers_enc = 2\n",
    "    num_layers_langs = 1\n",
    "    num_heads_enc = 3\n",
    "    dense_dim_enc = 256\n",
    "    drop_enc = 0.1\n",
    "    drop_dec = 0.2\n",
    "    dense_dim_dec = 1024\n",
    "    num_heads_dec = 5\n",
    "    num_layers_dec = 4\n",
    "    \n",
    "    neighbors_encoder_input = keras.Input(\n",
    "        shape=(max_num_lang, max_seq_len), dtype=\"int64\", name=\"neighbor_langs_forms\"\n",
    "    )\n",
    "\n",
    "    langs_encoder_input = keras.Input(\n",
    "        shape=(1),\n",
    "        dtype=\"int64\",\n",
    "        name=\"target_langs\",\n",
    "    )\n",
    "\n",
    "    form_pos_embed = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=vocab_size_forms,\n",
    "        sequence_length=max_seq_len,\n",
    "        embedding_dim=embedding_dim_form,\n",
    "        mask_zero=True,\n",
    "        name=\"form_embedding\",\n",
    "    )\n",
    "\n",
    "    seqs_by_lang = tf.unstack(neighbors_encoder_input, axis=1)\n",
    "    embedded_lst = []\n",
    "    for seq in seqs_by_lang:\n",
    "        embed_neighbor_form = form_pos_embed(seq)\n",
    "        embed_neighbor_form = Dropout(embed_drop)(embed_neighbor_form)\n",
    "        embedded_lst.append(embed_neighbor_form)\n",
    "\n",
    "    embed_target_langs = Embedding(\n",
    "        input_dim=vocab_size_langs,\n",
    "        output_dim=embeeding_dim_lang,\n",
    "        mask_zero=True,\n",
    "        embeddings_regularizer=keras.regularizers.OrthogonalRegularizer(\n",
    "            factor=0.05, mode=\"rows\"\n",
    "        ),\n",
    "        name=\"langs_embedding\",\n",
    "    )(langs_encoder_input)\n",
    "\n",
    "    encoder_langs = Encoder(\n",
    "        num_layers=num_layers_langs,\n",
    "        num_heads=num_heads_langs,\n",
    "        hidden_dim=dense_dim_langs,\n",
    "        dropout_rate=drop_enc,\n",
    "        name=\"target_langs_encoder\",\n",
    "    )\n",
    "    encoder_target_langs = encoder_langs(embed_target_langs)\n",
    "\n",
    "    embedded_input_concat = Concatenate(axis=1)(\n",
    "        [embed_neighbor for embed_neighbor in embedded_lst]\n",
    "    )\n",
    "\n",
    "    encoder_neighbor_forms = Encoder(\n",
    "        num_layers=num_layers_enc,\n",
    "        num_heads=num_heads_enc,\n",
    "        hidden_dim=dense_dim_enc,\n",
    "        dropout_rate=drop_enc,\n",
    "    )\n",
    "\n",
    "    encoder_out = encoder_neighbor_forms(embedded_input_concat)\n",
    "    # the following line is needed to ensure the possibility of concatenating the output of two encoders\n",
    "    encoder_target_langs = tf.repeat(\n",
    "        encoder_target_langs, tf.shape(encoder_out)[1], axis=1\n",
    "    )\n",
    "\n",
    "    combined_output = Concatenate(axis=-1)([encoder_target_langs, encoder_out])\n",
    "\n",
    "    decoder_target_form_input = keras.Input(\n",
    "        shape=(None,), dtype=\"int64\", name=\"target_langs_forms\"\n",
    "    )\n",
    "\n",
    "    embed_target_lang_form = form_pos_embed(decoder_target_form_input)\n",
    "    embed_target_lang_form = Dropout(embed_drop)(embed_target_lang_form)\n",
    "\n",
    "    decoder = Decoder(\n",
    "        num_layers=num_layers_dec,\n",
    "        num_heads=num_heads_dec,\n",
    "        hidden_dim=dense_dim_dec,\n",
    "        dropout_rate=drop_dec,\n",
    "    )\n",
    "\n",
    "    decoder_out = decoder(embed_target_lang_form, combined_output)\n",
    "\n",
    "    decoder_target_form_output = Dense(vocab_size_forms)(decoder_out)\n",
    "    neighbor_model = keras.Model(\n",
    "        [neighbors_encoder_input, langs_encoder_input, decoder_target_form_input],\n",
    "        decoder_target_form_output,\n",
    "    )\n",
    "    # Standard Sparse Categorical Crossentropy that additionally ignores masked elements in target sequence\n",
    "    def masked_loss(label, pred):\n",
    "        mask = label != 0\n",
    "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=\"none\"\n",
    "        )\n",
    "        loss = loss_object(label, pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "\n",
    "        loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    init_lr = 0.0005\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=50000,\n",
    "        decay_rate=0.5,\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98\n",
    "    )\n",
    "\n",
    "    neighbor_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=masked_loss,\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return neighbor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_neighbor_model = build_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"train_0.10.keras\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        mode=\"max\",\n",
    "        monitor=\"val_sparse_categorical_accuracy\",\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_sparse_categorical_accuracy\", patience=5\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Note that the number of epochs is excessive but due to use of callbacks, most realistically the model will stop around epoch 15\n",
    "num_epochs = 50\n",
    "multilingual_neighbor_model.fit(\n",
    "    train_dataset_batched,\n",
    "    validation_data=validation_dataset_batched,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is overfitted by the time it gets to the last epoch, we need to load the last checkpoint with the best value for validation sparse categorical accuracy. Note that The exact state of the optimizer doesn't matter, as we will only use the model to predict the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )\n",
    "    loss = loss_object(label, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "neighbor_model = keras.models.load_model(\n",
    "    \"train_0.10.keras\",\n",
    "    custom_objects={\n",
    "        \"TokenAndPositionEmbedding\": keras_nlp.layers.TokenAndPositionEmbedding, # may not be required, depending on tensorflow/keras_nlp version\n",
    "        \"TransformerEncoder\": keras_nlp.layers.TransformerEncoder, # may not be required, depending on tensorflow/keras_nlp version\n",
    "        \"TransformerDecoder\": keras_nlp.layers.TransformerDecoder, # may not be required, depending on tensorflow/keras_nlp version\n",
    "        \"Encoder\": Encoder,\n",
    "        \"Decoder\": Decoder,\n",
    "    },\n",
    "    compile=False,\n",
    ")\n",
    "neighbor_model.compile(loss=masked_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_langs = []\n",
    "target_langs_forms = []\n",
    "neighbor_langs_forms = []\n",
    "family_dict = {}\n",
    "for dir in lst_dirs:\n",
    "    with open(\"../ST2022/data/\" + dir + \"/test-0.10.tsv\", encoding=\"UTF-8\") as f, open(\n",
    "        \"../ST2022/data/\" + dir + \"/solutions-0.10.tsv\",\n",
    "        encoding=\"UTF-8\",\n",
    "    ) as f_sol:\n",
    "        file = f.readlines()\n",
    "        file_sol = f_sol.readlines()\n",
    "        data = list(map(lambda x: x.strip(\"\\n\").split(\"\\t\")[1:], file))\n",
    "        data_sol = list(map(lambda x: x.strip(\"\\n\").split(\"\\t\")[1:], file_sol))\n",
    "    lang_names = data[0]\n",
    "    for lang in lang_names:\n",
    "        family_dict[lang] = dir  # will be needed during metrics computation\n",
    "    for i in range(1, len(data)):\n",
    "        to_pred = data[i].index(\"?\")\n",
    "        target_langs_forms.append(\n",
    "            f\"[start] {lang_names[to_pred]} {data_sol[i][to_pred]} [end]\"\n",
    "        )\n",
    "        target_langs.append(lang_names[to_pred])\n",
    "        neighbor_langs_forms.append(\n",
    "            [\n",
    "                f\"{lang_names[j]} {data[i][j]}\"\n",
    "                for j in range(len(data[i]))\n",
    "                if j != to_pred\n",
    "            ]\n",
    "        )\n",
    "\n",
    "int_target_lang = lang_vectorization(target_langs)\n",
    "\n",
    "### Vectorize target sequence\n",
    "int_target_lang_form = form_vectorization(target_langs_forms)\n",
    "### Vectorize and pad\n",
    "int_neighbor_lang_form = []\n",
    "for neighborhood in neighbor_langs_forms:\n",
    "    int_neighborhood = form_vectorization(neighborhood)\n",
    "    num_neighbors = int_neighborhood.shape[0]\n",
    "    if num_neighbors < max_num_lang:\n",
    "        to_add = max_num_lang - num_neighbors\n",
    "        padding = tf.constant([[0, to_add], [0, 0]], dtype=tf.int32)\n",
    "        int_neighborhood = tf.pad(int_neighborhood, padding)\n",
    "    int_neighbor_lang_form.append(int_neighborhood)\n",
    "\n",
    "int_neighbor_lang_form = tf.convert_to_tensor(int_neighbor_lang_form)\n",
    "\n",
    "\n",
    "lang_form_vocab = form_vectorization.get_vocabulary()\n",
    "lang_form_lookup = dict(zip(range(len(lang_form_vocab)), lang_form_vocab))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\n",
    "            \"neighbor_langs_forms\": int_neighbor_lang_form[:, :, :-1],\n",
    "            \"target_langs\": int_target_lang,\n",
    "            \"target_langs_forms\": int_target_lang_form[:, :-1],\n",
    "        },\n",
    "        int_target_lang_form[:, 1:],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes long time and requires additional optimization. All four files with decoded target forms are provided in the repository. The function takes especially long in case of proportion with 50% of data retained for testing. Note that despite ```test_dataset``` containing target sequences, we do not use them in any way for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_decode_forms(test_dataset):\n",
    "    filename = open(\"predicted_train_0.10.tsv\", \"w\", encoding=\"UTF-8\")\n",
    "    print(\"Predicted\\tActual\", file=filename)\n",
    "    decoded_forms = []\n",
    "    total_samples = len(test_dataset)\n",
    "    progress_bar = tqdm(total=total_samples, desc=\"Decoding Forms\", unit=\"sample\")\n",
    "    count = 0\n",
    "    for input, target in test_dataset.take(-1):\n",
    "        neighbor_langs_forms = tf.expand_dims(input[\"neighbor_langs_forms\"], axis=0)\n",
    "        target_langs = tf.expand_dims(input[\"target_langs\"], axis=0)\n",
    "        decoded_form = \"[start]\"\n",
    "\n",
    "        for i in range(max_seq_len):\n",
    "            tokenized_target_form = form_vectorization([decoded_form])[:, :-1]\n",
    "            predictions = neighbor_model(\n",
    "                [neighbor_langs_forms, target_langs, tokenized_target_form]\n",
    "            )\n",
    "            sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "            sampled_token = lang_form_lookup[sampled_token_index]\n",
    "            decoded_form += \" \" + sampled_token\n",
    "            if sampled_token == \"[end]\":\n",
    "                break\n",
    "\n",
    "        decoded_forms.append(decoded_form)\n",
    "        target_decoded = \"[start]\"\n",
    "\n",
    "        for ind in target:\n",
    "            target_decoded += \" \" + lang_form_lookup[int(ind)]\n",
    "\n",
    "            if lang_form_lookup[int(ind)] == \"[end]\":\n",
    "                break\n",
    "\n",
    "        print(f\"{decoded_form}\\t{target_decoded}\", file=filename)\n",
    "\n",
    "        if count % 25 == 0:\n",
    "            print(f\"{decoded_form}\\t{target_decoded}\")\n",
    "        count += 1\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    filename.close()\n",
    "    progress_bar.close()\n",
    "\n",
    "predict_decode_forms(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sigtypst2022 import compare_words # this function is provided by the package that came with the shared task\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There was an important error in `strip_start_end_lang` function. It is now solved. I will not change the results in the paper but it slightly improves them. Essentially, in the initial version I was accidentally removing some additional characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_forms = pd.read_csv(\n",
    "    \"predicted_train_0.10.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"UTF-8\",\n",
    ")\n",
    "\n",
    "def strip_start_end_lang(inp_str):\n",
    "    new_string_lst = inp_str.split()\n",
    "    final_string = \" \".join(new_string_lst[2:-1])  # to remove lang name and [start], [end]\n",
    "    return final_string\n",
    "\n",
    "decoded_forms = decoded_forms.Predicted\n",
    "decoded_forms = list(decoded_forms)\n",
    "decoded_forms = list(map(lambda x: strip_start_end_lang(x), decoded_forms)) # remove [start] and [end] tokens, as well as languages from target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Where to store predictions\n",
    "dirs_preds = {\n",
    "    dir: f\"../ST2022/data/{dir}/result_train_0.10.tsv\"\n",
    "    for dir in set(family_dict.values())\n",
    "}\n",
    "\n",
    "forms_by_fam = {}\n",
    "for form, lang in zip(decoded_forms, target_langs):\n",
    "    family = family_dict[lang]\n",
    "    if family not in forms_by_fam.keys():\n",
    "        forms_by_fam[family] = []\n",
    "    forms_by_fam[family].append(form)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code works slightly strangely, but it simply takes an existing file with test data and simply wipes out all data there. It was more convenient than to create an empty file in a different way. Ultimately, it produces files in required format for comparison using function ```compare_words``` that was previously loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in lst_dirs:\n",
    "    test_data = pd.read_csv(\n",
    "        f\"../ST2022/data/{dir}/test-0.10.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"UTF-8\",\n",
    "    )\n",
    "    solutions = test_data.copy()\n",
    "    solutions.iloc[:, 1:] = \"\"\n",
    "\n",
    "    for language in test_data.columns[1:]:\n",
    "        language_ind = test_data[test_data[language] == \"?\"].index\n",
    "        slice_min, slice_max = min(language_ind), max(language_ind) + 1\n",
    "        solutions.loc[language_ind, language] = forms_by_fam[dir][slice_min:slice_max]\n",
    "    file_path = dirs_preds[dir]\n",
    "    solutions.to_csv(file_path, sep=\"\\t\", encoding=\"UTF-8\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ```compare_words``` returns a list of results. The final element of the list shows the results averaged per entire language family (as opposed to averaged by language). In the list of results, the final lists consists of several numbers which are metrics values. The following list says which metric correspond to which index of the final list with averaged results per language family:\n",
    "\n",
    "1 - Edit Distance\n",
    "\n",
    "2 - Normalized edit distance\n",
    "\n",
    "3 - B-Cubed F-scores\n",
    "\n",
    "4 - BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result for abrahammonpa is: 0.1617\n",
      "The result for allenbai is: 0.2456\n",
      "The result for backstromnorthernpakistan is: 0.2347\n",
      "The result for castrosui is: 0.0735\n",
      "The result for davletshinaztecan is: 0.3955\n",
      "The result for felekesemitic is: 0.3946\n",
      "The result for hantganbangime is: 0.3791\n",
      "The result for hattorijaponic is: 0.3556\n",
      "The result for listsamplesize is: 0.5351\n",
      "The result for mannburmish is: 0.5639\n",
      "The average result across all datasets is:  0.3339\n"
     ]
    }
   ],
   "source": [
    "\n",
    "average = []\n",
    "for dir in lst_dirs:\n",
    "    result = compare_words(\n",
    "        f\"../ST2022/data/{dir}/result_train_0.10.tsv\",\n",
    "        f\"../ST2022/data/{dir}/solutions-0.10.tsv\",\n",
    "        report=False,\n",
    "    )[-1][1]\n",
    "    print(f\"The result for {dir} is: {round(result, 4)}\")\n",
    "    average.append(result)\n",
    "print(\n",
    "    \"The average result across all datasets is: \", round(sum(average) / len(average), 4)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
